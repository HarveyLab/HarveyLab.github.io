<!-- The image that will be displayed next to the publication info: -->
<img class="thumbnail" src="img/bohnslav2020.PNG" id="Bohnslav2020">

<p class="publicationTitle">DeepEthogram: a machine learning pipeline for supervised behavior classification from raw pixels
</p>

<!-- Do not include the title in this citation, since that would be redundant with the title above. -->
<p class="publicationCitation">
	<!-- Make sure to include http:// in the link, otherwise it will be interpreted as a relative link. -->
	<a href="https://www.biorxiv.org/content/10.1101/2020.09.24.312504v1">
	Bohnslav, James P, Nivanthika K Wimalasena, Kelsey J Clausing, David Yarmolinsky, Tomas Cruz, Eugenia Chiappe, Lauren L Orefice, Clifford J Woolf, and Christopher D Harvey. (2020). bioRxiv 2020.09.24.312504.
	</a>
</p>

<!-- Case matters in the pdf file name. -->
<span class="buttonPdf" id="Bohnslav2020"><a href="pdf/bohnslav2020.pdf">Download PDF</a></span>

<div class="less">
	<span class="buttonShowMore" id="Bohnslav2020">Show abstract</span>
</div>

<!-- Make sure to replace special characters in the abstract with their HTML versions. -->
<div class="more">
	<p class="publicationAbstract">
	Researchers commonly acquire videos of animal behavior and quantify the prevalence of behaviors of interest to study nervous system function, the effects of gene mutations, and the efficacy of pharmacological therapies. This analysis is typically performed manually and is therefore immensely time consuming, often limited to a small number of behaviors, and variable across researchers. Here, we created DeepEthogram: software that takes raw pixel values of videos as input and uses machine learning to output an ethogram, the set of user-defined behaviors of interest present in each frame of a video. We used convolutional neural network models that compute motion in a video, extract features from motion and single frames, and classify these features into behaviors. These models classified behaviors with greater than 90% accuracy on single frames in videos of flies and mice, matching expert-level human performance. The models accurately predicted even extremely rare behaviors, required little training data, and generalized to new videos and subjects. DeepEthogram runs rapidly on common scientific computer hardware and has a graphical user interface that does not require programming by the end-user. We anticipate DeepEthogram will enable the rapid, automated, and reproducible assignment of behavior labels to every frame of a video, thus accelerating all those studies that quantify behaviors of interest. Code is available at: https://github.com/jbohnslav/deepethogram<span class="buttonShowLess" id="Bohnslav2020">Hide abstract</span>
	</p>
</div>




